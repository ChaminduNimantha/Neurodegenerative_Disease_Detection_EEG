{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2tu2jCHDDWQ"
      },
      "source": [
        "Install necessary libraries `scipy` for scientific computing and `seaborn` for data visualization capabilities.Optuna for hyperparameter tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kwTS4erADHKP",
        "outputId": "07863f50-4347-46ce-f9a0-2030b3b00567"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (1.13.1)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (0.13.1)\n",
            "Requirement already satisfied: numpy<2.3,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from scipy) (1.26.4)\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.10/dist-packages (from seaborn) (2.1.4)\n",
            "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /usr/local/lib/python3.10/dist-packages (from seaborn) (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2->seaborn) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2->seaborn) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "# Install Required Libraries\n",
        "!pip install scipy seaborn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73xgJmm9DPLh"
      },
      "source": [
        "Mount Google Drive to access the data files stored there"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cj9IeXqFDQaq",
        "outputId": "126cf918-5fb8-49c7-b724-6124fa6529d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive to access data files\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWEkxShVDb5a"
      },
      "source": [
        "Import the required libraries. `numpy` for numerical operations, `scipy.io` for loading MATLAB files, `sklearn` for machine learning model creation, training, and evaluation, `matplotlib.pyplot` and `seaborn` for data visualization. Other libraries for ML architectures and for evaluating them"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6z6jd2diDfWY"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import scipy.io\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy.stats import ttest_ind"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6IxUQ9IsDkB8"
      },
      "source": [
        "Load the coherence data from a MATLAB (.mat) file stored in Google Drive. It contains the coherence matrices for Alzheimer's Disease (AZ), Frontotemporal Dementia (FTD), and Healthy Controls (HC). Use `scipy.io.loadmat` to read the .mat file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R8406IZGDo20"
      },
      "outputs": [],
      "source": [
        "# Load the coherence data from .mat file\n",
        "mat_file_path = '/content/drive/MyDrive/GNN_Approach/Data_files/Coherence.mat'\n",
        "mat = scipy.io.loadmat(mat_file_path)\n",
        "coherence_data = mat['Coherence'][0, 0]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy.io\n",
        "import numpy as np\n",
        "\n",
        "# Load the .mat file\n",
        "file_path = '/content/drive/MyDrive/GNN_Approach/Subjects/Subject1.mat'\n",
        "mat_data = scipy.io.loadmat(file_path)\n",
        "\n",
        "# Extract the TF matrix\n",
        "TF = mat_data['TF']\n",
        "\n",
        "# Define the submatrices as specified\n",
        "submatrix1 = TF[:, :, 0:8]\n",
        "submatrix2 = TF[:, :, 8:15]\n",
        "submatrix3 = TF[:, :, 15:27]\n",
        "submatrix4 = TF[:, :, 27:60]\n",
        "submatrix5 = TF[:, :, 60:92]\n",
        "\n",
        "# Compute the average along the third dimension for each submatrix\n",
        "averageMatrix1 = np.mean(submatrix1, axis=2)\n",
        "averageMatrix2 = np.mean(submatrix2, axis=2)\n",
        "averageMatrix3 = np.mean(submatrix3, axis=2)\n",
        "averageMatrix4 = np.mean(submatrix4, axis=2)\n",
        "averageMatrix5 = np.mean(submatrix5, axis=2)\n",
        "\n",
        "# Combine the averages into a new matrix with the shape 2346*1*5\n",
        "averageMatrix_combined = np.stack([averageMatrix1, averageMatrix2, averageMatrix3, averageMatrix4, averageMatrix5], axis=2)\n",
        "\n",
        "# Reshape the matrix from 2346*1*5 to 2346*5*1\n",
        "averageMatrix_reshaped = np.transpose(averageMatrix_combined, (0, 2, 1))\n",
        "\n",
        "# Now, averageMatrix_reshaped has the shape 2346*5*1\n",
        "# print(averageMatrix_reshaped.shape)\n",
        "\n",
        "Patient_matrix = averageMatrix_reshaped\n"
      ],
      "metadata": {
        "id": "Yv6GVslLOdCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pJh8VRwDsxz"
      },
      "source": [
        "Extract the Coherence data for different categories: Alzheimer's Disease (AZ), Frontotemporal Dementia (FTD), and Healthy Controls (HC)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nIBbiGnsDuAT"
      },
      "outputs": [],
      "source": [
        "# Extracting data in different categories (AZ, FTD, HC)\n",
        "coherence_AZ_F = coherence_data['AZ']\n",
        "coherence_FTD_F = coherence_data['FTD']\n",
        "coherence_HC_F = coherence_data['HC']\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwhI6OL6Dx_P"
      },
      "source": [
        "Extract the data corresponding to the 3rd frequency band (index 2) and then reshape the extracted data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fqPn8qL9D2Yr"
      },
      "outputs": [],
      "source": [
        "# Extract the 3rd frequency band (index 2) and normalize the data\n",
        "coherence_AZ = coherence_AZ_F[:, : , :].reshape(2346, 5, 36)\n",
        "coherence_FTD = coherence_FTD_F[:, : , :].reshape(2346, 5, 23)\n",
        "coherence_HC = coherence_HC_F[:, : , :].reshape(2346, 5, 29)\n",
        "\n",
        "# Optional: To test whether the data is converted properly into numpy arrays\n",
        "# print(coherence_AZ.shape)\n",
        "# print(coherence_FTD.shape)\n",
        "# print(coherence_HC.shape)\n",
        "# print(Patient_matrix.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create Feature mask with given P values for 3 Categories. Here you can give the P values to select the features which are used to masking the connectivity data"
      ],
      "metadata": {
        "id": "e3yt3k6qcApN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Reshape data to (2346, 5, 36), (2346, 5, 29), (2346, 5, 23)\n",
        "fm_data_az = coherence_AZ.reshape(2346, 5, 36)\n",
        "fm_data_hc = coherence_HC.reshape(2346, 5, 29)\n",
        "fm_data_ftd = coherence_FTD.reshape(2346, 5, 23)\n",
        "\n",
        "alpha_az_hc = 0.01\n",
        "alpha_ftd_hc = 0.01\n",
        "alpha_az_ftd = 0.05\n",
        "\n",
        "p_values_az_hc = np.zeros((fm_data_az.shape[0], fm_data_az.shape[1]))\n",
        "p_values_ftd_hc = np.zeros((fm_data_ftd.shape[0], fm_data_ftd.shape[1]))\n",
        "p_values_az_ftd = np.zeros((fm_data_az.shape[0], fm_data_az.shape[1]))\n",
        "\n",
        "# Calculate p-values for AZ vs HC\n",
        "for i in range(fm_data_az.shape[0]):\n",
        "    for j in range(fm_data_az.shape[1]):\n",
        "        _, p_value = ttest_ind(fm_data_az[i, j, :], fm_data_hc[i, j, :], equal_var=False) # welch's t test (Varient of Independent two sample t test)\n",
        "        p_values_az_hc[i, j] = p_value\n",
        "\n",
        "# Calculate p-values for FTD vs HC\n",
        "for i in range(fm_data_ftd.shape[0]):\n",
        "    for j in range(fm_data_ftd.shape[1]):\n",
        "        _, p_value = ttest_ind(fm_data_ftd[i, j, :], fm_data_hc[i, j, :], equal_var=False)\n",
        "        p_values_ftd_hc[i, j] = p_value\n",
        "\n",
        "# Calculate p-values for AZ vs FTD\n",
        "for i in range(fm_data_az.shape[0]):\n",
        "    for j in range(fm_data_az.shape[1]):\n",
        "        _, p_value = ttest_ind(fm_data_az[i, j, :], fm_data_ftd[i, j, :], equal_var=False)\n",
        "        p_values_az_ftd[i, j] = p_value\n",
        "\n",
        "# Create boolean masks\n",
        "mask_az_hc = (p_values_az_hc < alpha_az_hc).astype(int).reshape(2346, 5, 1)\n",
        "mask_ftd_hc = (p_values_ftd_hc < alpha_ftd_hc).astype(int).reshape(2346, 5, 1)\n",
        "mask_az_ftd = (p_values_az_ftd < alpha_az_ftd).astype(int).reshape(2346, 5, 1)\n",
        "\n",
        "# print(\"Shapes of masks:\")\n",
        "# print(mask_az_hc.shape)\n",
        "# print(mask_ftd_hc.shape)\n",
        "# print(mask_az_ftd.shape)\n",
        "\n",
        "def calculate_significant_percentage(feature_mask):\n",
        "    total_features = feature_mask.size\n",
        "    significant_features = np.sum(feature_mask)\n",
        "    percentage = (significant_features / total_features) * 100\n",
        "    return percentage\n",
        "\n",
        "\n",
        "# Calculate and print percentage of significant features for individual masks\n",
        "# percentage_significant_AZ_HC = calculate_significant_percentage(mask_az_hc)\n",
        "# print(\"AZ_HC Feature mask):\")\n",
        "# print(mask_az_hc.shape)\n",
        "# print(f\"Percentage of significant features AZ vs HC : {percentage_significant_AZ_HC:.2f}%\")\n",
        "\n",
        "# percentage_significant_FTD_HC = calculate_significant_percentage(mask_ftd_hc)\n",
        "# print(\"FTD_HC Feature mask):\")\n",
        "# print(mask_ftd_hc.shape)\n",
        "# print(f\"Percentage of significant features FTD vs HC : {percentage_significant_FTD_HC:.2f}%\")\n",
        "\n",
        "# percentage_significant_AZ_FTD = calculate_significant_percentage(mask_az_ftd)\n",
        "# print(\"AZ_FTD Feature mask):\")\n",
        "# print(mask_az_ftd.shape)\n",
        "# print(f\"Percentage of significant features AZ vs FTD  : {percentage_significant_AZ_FTD:.2f}%\")\n",
        "\n",
        "# Create combined feature mask\n",
        "combined_feature_mask = ((mask_az_hc + mask_ftd_hc + mask_az_ftd) > 0).astype(int)\n",
        "\n",
        "# Calculate and print percentage of significant features for combined mask\n",
        "# percentage_significant_combined = calculate_significant_percentage(combined_feature_mask)\n",
        "# print(\"Combined Feature mask:\")\n",
        "# print(combined_feature_mask.shape)\n",
        "# print(f\"Percentage of significant features in combined mask: {percentage_significant_combined:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RFEOShi5m4_1",
        "outputId": "f4897a58-b7d4-45a5-8485-567c09415a94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/scipy/stats/_axis_nan_policy.py:531: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
            "  res = hypotest_fun_out(*samples, **kwds)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shapes of masks:\n",
            "(2346, 5, 1)\n",
            "(2346, 5, 1)\n",
            "(2346, 5, 1)\n",
            "AZ_HC Feature mask):\n",
            "(2346, 5, 1)\n",
            "Percentage of significant features AZ vs HC : 11.24%\n",
            "FTD_HC Feature mask):\n",
            "(2346, 5, 1)\n",
            "Percentage of significant features FTD vs HC : 4.68%\n",
            "AZ_FTD Feature mask):\n",
            "(2346, 5, 1)\n",
            "Percentage of significant features AZ vs FTD  : 4.50%\n",
            "Combined Feature mask:\n",
            "(2346, 5, 1)\n",
            "Percentage of significant features in combined mask: 16.05%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Band Filter for the above Features obtained with different P values. set the bands you dont want and make them into zero"
      ],
      "metadata": {
        "id": "fANL9wvi48EJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bands_to_zero = []\n",
        "\n",
        "mask_az_hc[:, bands_to_zero, :] = 0\n",
        "mask_ftd_hc[:, bands_to_zero, :] = 0\n",
        "mask_az_ftd[:, bands_to_zero, :] = 0\n",
        "\n",
        "# Create combined feature mask\n",
        "combined_feature_mask = ((mask_az_hc + mask_ftd_hc + mask_az_ftd) > 0).astype(int)\n",
        "\n",
        "# # Calculate and print percentage of significant features for individual masks\n",
        "# percentage_significant_AZ_HC = calculate_significant_percentage(mask_az_hc)\n",
        "# print(\"AZ_HC Feature mask):\")\n",
        "# print(mask_az_hc.shape)\n",
        "# print(f\"Percentage of significant features AZ vs HC : {percentage_significant_AZ_HC:.2f}%\")\n",
        "\n",
        "# percentage_significant_FTD_HC = calculate_significant_percentage(mask_ftd_hc)\n",
        "# print(\"FTD_HC Feature mask):\")\n",
        "# print(mask_ftd_hc.shape)\n",
        "# print(f\"Percentage of significant features FTD vs HC : {percentage_significant_FTD_HC:.2f}%\")\n",
        "\n",
        "# percentage_significant_AZ_FTD = calculate_significant_percentage(mask_az_ftd)\n",
        "# print(\"AZ_FTD Feature mask):\")\n",
        "# print(mask_az_ftd.shape)\n",
        "# print(f\"Percentage of significant features AZ vs FTD  : {percentage_significant_AZ_FTD:.2f}%\")\n",
        "\n",
        "# # Calculate and print percentage of significant features for combined mask\n",
        "# percentage_significant_combined = calculate_significant_percentage(combined_feature_mask)\n",
        "# print(\"Combined Feature mask:\")\n",
        "# print(combined_feature_mask.shape)\n",
        "# print(f\"Percentage of significant features in combined mask: {percentage_significant_combined:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "800zEbUJCFAd",
        "outputId": "5be30bd2-b179-4e6e-8077-ed67c58c1b97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AZ_HC Feature mask):\n",
            "(2346, 5, 1)\n",
            "Percentage of significant features AZ vs HC : 11.24%\n",
            "FTD_HC Feature mask):\n",
            "(2346, 5, 1)\n",
            "Percentage of significant features FTD vs HC : 4.68%\n",
            "AZ_FTD Feature mask):\n",
            "(2346, 5, 1)\n",
            "Percentage of significant features AZ vs FTD  : 4.50%\n",
            "Combined Feature mask:\n",
            "(2346, 5, 1)\n",
            "Percentage of significant features in combined mask: 16.05%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-nE9biVD9Kj"
      },
      "source": [
        "Reconstructs the coherence matrix from the extracted coherence data. The function iterates over frequency bands and subjects, creating a 68x68 matrix with coherence values. The rebuilt matrices are stored in a 4D array with dimensions corresponding to regions, regions, bands, and subjects"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cuG4hur9EAY0"
      },
      "outputs": [],
      "source": [
        "# Function to rebuild the coherence matrix from the coherence data\n",
        "def rebuild_matrix(coherence_data):\n",
        "    n_regions = 68\n",
        "    n_data = coherence_data.shape[2]\n",
        "    n_bands = coherence_data.shape[1]\n",
        "\n",
        "    rebuilt_data = np.zeros((n_regions, n_regions, n_bands, n_data))\n",
        "\n",
        "    for band in range(n_bands):\n",
        "        for subject in range(n_data):\n",
        "            matrix = np.zeros((n_regions, n_regions))\n",
        "            idx = 0\n",
        "            for col in range(n_regions):\n",
        "                for row in range(col + 1):\n",
        "                    matrix[row, col] = coherence_data[idx, band, subject]\n",
        "                    idx += 1\n",
        "            matrix = matrix + matrix.T - np.diag(matrix.diagonal())\n",
        "            rebuilt_data[:, :, band, subject] = matrix\n",
        "\n",
        "    return rebuilt_data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reconstruct Mask Matrix"
      ],
      "metadata": {
        "id": "fG1j8-eq7Dcq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to rebuild the Feature mask matrix from mask data\n",
        "\n",
        "def rebuild_matrix_mask(mask_data):\n",
        "    n_regions = 68\n",
        "    # n_data = coherence_data.shape[2]\n",
        "    n_data = 1\n",
        "    n_bands = mask_data.shape[1]\n",
        "\n",
        "    rebuilt_data = np.zeros((n_regions, n_regions, n_bands, n_data))\n",
        "\n",
        "    for band in range(n_bands):\n",
        "        for subject in range(n_data):\n",
        "            matrix = np.zeros((n_regions, n_regions))\n",
        "            idx = 0\n",
        "            for col in range(n_regions):\n",
        "                for row in range(col + 1):\n",
        "                    matrix[row, col] = mask_data[idx, band, subject]\n",
        "                    idx += 1\n",
        "            matrix = matrix + matrix.T - np.diag(matrix.diagonal())\n",
        "            rebuilt_data[:, :, band, subject] = matrix\n",
        "\n",
        "    return rebuilt_data"
      ],
      "metadata": {
        "id": "LT63cB4KoQ7k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqboKkriED5X"
      },
      "source": [
        "Reconstruct the coherence matrices for each category (AZ, FTD, HC) and check the shapes of the resulting matrices to ensure that they have been reconstructed correctly and have the expected dimensions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0D9VZFmUEG97"
      },
      "outputs": [],
      "source": [
        "# Rebuild the coherence matrices\n",
        "coherence_AZ_rebuilt = rebuild_matrix(coherence_AZ)\n",
        "coherence_FTD_rebuilt = rebuild_matrix(coherence_FTD)\n",
        "coherence_HC_rebuilt = rebuild_matrix(coherence_HC)\n",
        "Patient_rebuilt = rebuild_matrix(Patient_matrix)\n",
        "\n",
        "# Check the shapes to ensure correctness\n",
        "# print('COH_AZ_rebuilt shape:', coherence_AZ_rebuilt.shape)\n",
        "# print('COH_FTD_rebuilt shape:', coherence_FTD_rebuilt.shape)\n",
        "# print('COH_HC_rebuilt shape:', coherence_HC_rebuilt.shape)\n",
        "# print('Patient_rebuilt shape:', Patient_rebuilt.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mask"
      ],
      "metadata": {
        "id": "Y-8PD1bz7Lam"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rebuild the coherence matrices\n",
        "combined_mask_rebuilt = rebuild_matrix_mask(combined_feature_mask)\n",
        "\n",
        "# Check the shapes to ensure correctness\n",
        "# print('combined_mask_rebuilt shape:', combined_mask_rebuilt.shape)\n",
        "\n",
        "count_combined_mask_rebuilt  = np.sum(combined_mask_rebuilt)\n",
        "# print(\"Number of True values in combined mask:\", count_combined_mask_rebuilt)\n",
        "\n",
        "# print(combined_mask_rebuilt[:,:,2,0])"
      ],
      "metadata": {
        "id": "2_PfgG-MxWxM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUJu2PQsEYL5"
      },
      "source": [
        "Extracts edge features from the coherence matrices and assigns labels to them. The function iterates over the subjects and bands, extracting the upper triangular part of each coherence matrix (excluding the diagonal). These edge attributes are then stored in a feature array, and the corresponding labels are stored in a label array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-yRSEPdtoYOo"
      },
      "outputs": [],
      "source": [
        "def extract_edge_features_labels(data, label):\n",
        "    features = []\n",
        "    labels = []\n",
        "\n",
        "    # Iterate over each sample\n",
        "    for i in range(data.shape[3]):  # Samples dimension (36)\n",
        "        combined_edge_attr = []\n",
        "\n",
        "        # Iterate over each frequency band\n",
        "        for j in range(data.shape[2]):  # Frequency bands dimension (5)\n",
        "            matrix = data[:, :, j, i]  # Extract the 68x68 matrix for the j-th frequency band and i-th sample\n",
        "            edge_attr = matrix[np.triu_indices_from(matrix, k=1)]  # Extract upper triangular part (excluding diagonal)\n",
        "            combined_edge_attr.extend(edge_attr)  # Combine features from all frequency bands\n",
        "\n",
        "        features.append(combined_edge_attr)  # Append combined features for the current sample\n",
        "        labels.append(label)  # Append the label for the current sample\n",
        "\n",
        "    return np.array(features), np.array(labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "own5lLWtEd10"
      },
      "source": [
        "Use the `extract_edge_features_labels` function to extract features and labels for different classification tasks:\n",
        "1. Alzheimer's Disease (AZ) vs. Healthy Controls (HC)\n",
        "2. Frontotemporal Dementia (FTD) vs. Healthy Controls (HC)\n",
        "3. Frontotemporal Dementia (FTD) vs. Alzheimer's Disease (AZ)\n",
        "\n",
        "For each classification task, extract the edge features and labels from the rebuilt coherence matrices and concatenate them to form the complete dataset for each classification task"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MyWgv62fEhgO"
      },
      "outputs": [],
      "source": [
        "# Extract features and labels for AZ vs HC\n",
        "features_az_hc, labels_az_hc = extract_edge_features_labels(coherence_HC_rebuilt, 0)\n",
        "features_az_hc_, labels_az_hc_ = extract_edge_features_labels(coherence_AZ_rebuilt, 2)\n",
        "features_az_hc = np.concatenate((features_az_hc, features_az_hc_), axis=0)\n",
        "labels_az_hc = np.concatenate((labels_az_hc, labels_az_hc_), axis=0)\n",
        "\n",
        "# Extract features and labels for FTD vs HC\n",
        "features_ftd_hc, labels_ftd_hc = extract_edge_features_labels(coherence_HC_rebuilt, 0)\n",
        "features_ftd_hc_, labels_ftd_hc_ = extract_edge_features_labels(coherence_FTD_rebuilt, 1)\n",
        "features_ftd_hc = np.concatenate((features_ftd_hc, features_ftd_hc_), axis=0)\n",
        "labels_ftd_hc = np.concatenate((labels_ftd_hc, labels_ftd_hc_), axis=0)\n",
        "\n",
        "# Extract features and labels for FTD vs AZ\n",
        "features_ftd_az, labels_ftd_az = extract_edge_features_labels(coherence_FTD_rebuilt, 1)\n",
        "features_ftd_az_, labels_ftd_az_ = extract_edge_features_labels(coherence_AZ_rebuilt, 2)\n",
        "features_ftd_az = np.concatenate((features_ftd_az, features_ftd_az_), axis=0)\n",
        "labels_ftd_az = np.concatenate((labels_ftd_az, labels_ftd_az_), axis=0)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mask"
      ],
      "metadata": {
        "id": "PLQVGbt37bXF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract mask features and mask labels\n",
        "combined_features_masks, labels_mask_az_hc = extract_edge_features_labels(combined_mask_rebuilt, 7)\n",
        "combined_features_masks = combined_features_masks.astype(bool)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "On0dRXj8pHvZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Patient"
      ],
      "metadata": {
        "id": "tE47B5rhZ2yI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "features_patient,label_patient = extract_edge_features_labels(Patient_rebuilt, 8)"
      ],
      "metadata": {
        "id": "LhYh4pwpTn0X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature summary"
      ],
      "metadata": {
        "id": "Yst1q6nU7lxt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# print(features_az_hc)\n",
        "# print(features_az_hc.shape)\n",
        "# print(features_ftd_hc)\n",
        "# print(features_ftd_hc.shape)\n",
        "# print(features_ftd_az)\n",
        "# print(features_ftd_az.shape)\n",
        "# print(features_patient)\n",
        "# print(features_patient.shape)\n",
        "\n",
        "# print(combined_features_masks)\n",
        "# print(combined_features_masks.shape)\n",
        "\n",
        "\n",
        "# Count the number of True values\n",
        "count_combined_mask = np.sum(combined_features_masks)\n",
        "print(\"Number of True values in combined mask:\", count_combined_mask)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_iWEkOyvpNXw",
        "outputId": "232d568f-dfe5-49ee-80fb-f622f2fc4de1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of True values in combined mask: 1883\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature masking and selecting"
      ],
      "metadata": {
        "id": "MsBvPGJs7uyT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "combined_feature_mask_flat = combined_features_masks.flatten()\n",
        "# print(combined_feature_mask_flat)\n",
        "# print(combined_feature_mask_flat.shape)\n",
        "\n",
        "# Apply the mask to extract only the selected features\n",
        "masked_features_az_hc = features_az_hc[:, combined_feature_mask_flat]\n",
        "masked_features_ftd_hc = features_ftd_hc[:, combined_feature_mask_flat]\n",
        "masked_features_ftd_az = features_ftd_az[:, combined_feature_mask_flat]\n",
        "masked_features_patient = features_patient[:, combined_feature_mask_flat]\n",
        "\n",
        "# print(\"Original AZ_HC features shape:\", features_az_hc.shape)\n",
        "# print(\"Mask AZ_HC shape:\", combined_feature_mask_flat.shape)\n",
        "# print(\"Masked AZ_HC features shape:\", masked_features_az_hc.shape)\n",
        "\n",
        "# print(\"Original FTD_HC features shape:\", features_ftd_hc.shape)\n",
        "# print(\"Mask FTD_HC shape:\", combined_feature_mask_flat.shape)\n",
        "# print(\"Masked FTD_HC features shape:\", masked_features_ftd_hc.shape)\n",
        "\n",
        "# print(\"Original FTD_AZ features shape:\", features_ftd_az.shape)\n",
        "# print(\"Mask FTD_AZ shape:\", combined_feature_mask_flat.shape)\n",
        "# print(\"Masked FTD_AZ features shape:\", masked_features_ftd_az.shape)\n",
        "\n",
        "# print(\"Original Patient features shape:\", features_patient.shape)\n",
        "# print(\"Mask Patient shape:\", combined_feature_mask_flat.shape)\n",
        "# print(\"Masked Patient features shape:\", masked_features_patient.shape)\n"
      ],
      "metadata": {
        "id": "9xBSWT-WpZNv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0da4h-_8WDw4"
      },
      "source": [
        "Visualize the edge features of a coherence matrix. The function takes the features, label, and title as inputs and plots the edge features using Matplotlib anlog with the labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHnkBnbtEk4o"
      },
      "source": [
        "Split the extracted features and labels into training and testing sets for three classification tasks:\n",
        "1. Alzheimer's Disease (AZ) vs. Healthy Controls (HC)\n",
        "2. Frontotemporal Dementia (FTD) vs. Healthy Controls (HC)\n",
        "3. Frontotemporal Dementia (FTD) vs. Alzheimer's Disease (AZ)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qC1BhmVLEoSy"
      },
      "outputs": [],
      "source": [
        "# Split data into training and testing sets for AZ vs HC\n",
        "X_train_az_hc, X_test_az_hc, y_train_az_hc, y_test_az_hc = train_test_split(masked_features_az_hc, labels_az_hc, test_size=0.2, random_state=42)\n",
        "\n",
        "# Split data into training and testing sets for FTD vs HC\n",
        "X_train_ftd_hc, X_test_ftd_hc, y_train_ftd_hc, y_test_ftd_hc = train_test_split(masked_features_ftd_hc, labels_ftd_hc, test_size=0.2, random_state=42)\n",
        "\n",
        "# Split data into training and testing sets for FTD vs AZ\n",
        "X_train_ftd_az, X_test_ftd_az, y_train_ftd_az, y_test_ftd_az = train_test_split(masked_features_ftd_az, labels_ftd_az, test_size=0.2, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n",
        "# Define the SVC models with the provided parameters\n",
        "\n",
        "# svc_az_hc = SVC(C=2.1214906873874253, gamma=0.004484292508881285, kernel='rbf', probability=True)\n",
        "# {'C': 0.11270661472823854, 'gamma': 0.16554604989987254, 'kernel': 'poly'}\n",
        "# {'C': 0.08417824373866711, 'kernel': 'linear'}\n",
        "svc_az_hc = SVC(C=0.08417824373866711, kernel='linear', probability=True)\n",
        "\n",
        "\n",
        "# svc_ftd_hc = SVC(C=4.052602921806768, gamma=0.0042184837966682395, kernel='rbf', probability=True)\n",
        "# {'C': 0.03208690181351073, 'gamma': 0.18157193875483335, 'kernel': 'linear'}\n",
        "#  {'C': 1.6953850458743818, 'kernel': 'rbf', 'gamma': 'scale'}\n",
        "svc_ftd_hc = SVC(C= 1.6953850458743818, gamma='scale', kernel='rbf', probability=True)\n",
        "\n",
        "\n",
        "# svc_ftd_az = SVC(C=218.092808835883, kernel='sigmoid', gamma='scale', probability=True)\n",
        "\n",
        "# {'C': 233.74681216558378, 'kernel': 'sigmoid', 'gamma': 'scale'}\n",
        "# {'C': 256.2063292041088, 'gamma': 0.01425320509380262, 'kernel': 'sigmoid'}\n",
        "# {'C': 264.97543383155426, 'kernel': 'sigmoid', 'gamma': 'scale'}\n",
        "svc_ftd_az = SVC(C=256.2063292041088, kernel='sigmoid', gamma= 0.01425320509380262, probability=True)\n",
        "\n",
        "# Define datasets for each comparison in the desired order\n",
        "datasets = {\n",
        "    'AZ_vs_HC': (X_train_az_hc, y_train_az_hc, X_test_az_hc, y_test_az_hc),\n",
        "    'FTD_vs_HC': (X_train_ftd_hc, y_train_ftd_hc, X_test_ftd_hc, y_test_ftd_hc),\n",
        "    'AZ_vs_FTD': (X_train_ftd_az, y_train_ftd_az, X_test_ftd_az, y_test_ftd_az)\n",
        "}\n",
        "\n",
        "# Define the classifiers dictionary in the desired order\n",
        "classifiers = {\n",
        "    'AZ_vs_HC': svc_az_hc,\n",
        "    'FTD_vs_HC': svc_ftd_hc,\n",
        "    'AZ_vs_FTD': svc_ftd_az\n",
        "}\n",
        "\n",
        "# Initialize a dictionary to store the cross-validation results\n",
        "cv_results = {}\n",
        "\n",
        "# Loop through each comparison and perform cross-validation\n",
        "for comparison, clf in classifiers.items():\n",
        "    X_train, y_train, X_test, y_test = datasets[comparison]\n",
        "\n",
        "    # Perform cross-validation and store the results\n",
        "    scores = cross_val_score(clf, X_train, y_train, cv=10)\n",
        "    cv_results[comparison] = scores\n",
        "\n",
        "    # Evaluate the SVC model on the test set\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "\n",
        "    # Print results for each model\n",
        "    print(f\"\\nSummary of cross-validation results for {comparison}:\")\n",
        "    print(f\"{comparison}: Mean accuracy = {scores.mean():.4f}, Standard deviation = {scores.std():.4f}\")\n",
        "\n",
        "    print(f\"\\nClassification Report for {comparison}:\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "\n",
        "    print(f\"Confusion Matrix for {comparison}:\")\n",
        "    print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "    print(f\"Accuracy for {comparison}: {accuracy_score(y_test, y_pred):.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tFFONhUYM-fL",
        "outputId": "7ef3a14c-ef6a-4a72-e360-b739841a96b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Summary of cross-validation results for AZ_vs_HC:\n",
            "AZ_vs_HC: Mean accuracy = 0.8900, Standard deviation = 0.1174\n",
            "\n",
            "Classification Report for AZ_vs_HC:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.80      0.80         5\n",
            "           2       0.88      0.88      0.88         8\n",
            "\n",
            "    accuracy                           0.85        13\n",
            "   macro avg       0.84      0.84      0.84        13\n",
            "weighted avg       0.85      0.85      0.85        13\n",
            "\n",
            "Confusion Matrix for AZ_vs_HC:\n",
            "[[4 1]\n",
            " [1 7]]\n",
            "Accuracy for AZ_vs_HC: 0.8462\n",
            "\n",
            "Summary of cross-validation results for FTD_vs_HC:\n",
            "FTD_vs_HC: Mean accuracy = 0.8550, Standard deviation = 0.1193\n",
            "\n",
            "Classification Report for FTD_vs_HC:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      1.00      0.92         6\n",
            "           1       1.00      0.80      0.89         5\n",
            "\n",
            "    accuracy                           0.91        11\n",
            "   macro avg       0.93      0.90      0.91        11\n",
            "weighted avg       0.92      0.91      0.91        11\n",
            "\n",
            "Confusion Matrix for FTD_vs_HC:\n",
            "[[6 0]\n",
            " [1 4]]\n",
            "Accuracy for FTD_vs_HC: 0.9091\n",
            "\n",
            "Summary of cross-validation results for AZ_vs_FTD:\n",
            "AZ_vs_FTD: Mean accuracy = 0.7400, Standard deviation = 0.2691\n",
            "\n",
            "Classification Report for AZ_vs_FTD:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.71      1.00      0.83         5\n",
            "           2       1.00      0.71      0.83         7\n",
            "\n",
            "    accuracy                           0.83        12\n",
            "   macro avg       0.86      0.86      0.83        12\n",
            "weighted avg       0.88      0.83      0.83        12\n",
            "\n",
            "Confusion Matrix for AZ_vs_FTD:\n",
            "[[5 0]\n",
            " [2 5]]\n",
            "Accuracy for AZ_vs_FTD: 0.8333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjJWkicDFnfC"
      },
      "source": [
        "Training SVC models for AZ vs HC,FTD vs HC and AZ vs FTD using the best parameters obtained"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 'SVC': SVC(C=2.1214906873874253,gamma=0.004484292508881285,kernel='rbf',probability=True)\n",
        "# SVC(C=0.08417824373866711, kernel='linear', probability=True)\n",
        "svc_az_hc = SVC(C=0.08417824373866711, kernel='linear', probability=True)\n",
        "svc_az_hc.fit(X_train_az_hc, y_train_az_hc)\n",
        "\n",
        "\n",
        "#{'C': 218.092808835883, 'kernel': 'sigmoid', 'gamma': 'scale'}\n",
        "# SVC(C=256.2063292041088, kernel='sigmoid', gamma= 0.01425320509380262, probability=True)\n",
        "svc_ftd_az = SVC(C=256.2063292041088, kernel='sigmoid', gamma= 0.01425320509380262, probability=True)\n",
        "svc_ftd_az.fit(X_train_ftd_az, y_train_ftd_az)\n",
        "\n",
        "# 'SVC'= SVC(C=4.115770269418699,gamma=0.003852086974868314,kernel='poly',probability=True)\n",
        "#(C= 1.6953850458743818, gamma='scale', kernel='rbf', probability=True)\n",
        "svc_ftd_hc = SVC(C= 1.6953850458743818, gamma='scale', kernel='rbf', probability=True)\n",
        "svc_ftd_hc.fit(X_train_ftd_hc, y_train_ftd_hc)\n",
        "\n",
        "# print(X_train_ftd_hc.shape)\n",
        "# print(masked_features_patient.shape)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "fbfDolFZ3V3m",
        "outputId": "1029cbf7-b11a-4ec9-bb69-306d08ed0378"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVC(C=1.6953850458743818, probability=True)"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(C=1.6953850458743818, probability=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(C=1.6953850458743818, probability=True)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Combined model"
      ],
      "metadata": {
        "id": "DlzsEGA8BH4o"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TUM8InC-F2F5"
      },
      "outputs": [],
      "source": [
        "def combined_model(X):\n",
        "    predictions = []\n",
        "    for x in X:\n",
        "        x = x.reshape(1, -1)\n",
        "        pred_az_hc = svc_az_hc.predict(x)\n",
        "        pred_ftd_hc = svc_ftd_hc.predict(x)\n",
        "        if pred_az_hc == 0 and pred_ftd_hc == 0:\n",
        "            predictions.append(0)  # Healthy Control\n",
        "        else:\n",
        "            pred_ftd_az = svc_ftd_az.predict(x)\n",
        "            if pred_ftd_az == 1:\n",
        "                predictions.append(1)  # FTD\n",
        "            else:\n",
        "                predictions.append(2)  # AZ\n",
        "    return np.array(predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PvyvEVauF-Z7"
      },
      "outputs": [],
      "source": [
        "# Combine test sets and labels for final evaluation\n",
        "X_test_combined = np.concatenate((X_test_az_hc, X_test_ftd_hc, X_test_ftd_az), axis=0)\n",
        "y_test_combined = np.concatenate((y_test_az_hc, y_test_ftd_hc, y_test_ftd_az), axis=0)\n",
        "\n",
        "# Predict using the combined model\n",
        "y_pred_combined = combined_model(X_test_combined)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Classification Report for Combined Model"
      ],
      "metadata": {
        "id": "YBmYVvawBKfP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MgYONONTGTza",
        "outputId": "db3ae61c-3660-4730-cfc4-fc151ff7c1ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report for Combined Model:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.91      0.91        11\n",
            "           1       0.73      0.80      0.76        10\n",
            "           2       0.86      0.80      0.83        15\n",
            "\n",
            "    accuracy                           0.83        36\n",
            "   macro avg       0.83      0.84      0.83        36\n",
            "weighted avg       0.84      0.83      0.83        36\n",
            "\n",
            "Confusion Matrix for Combined Model:\n",
            "[[10  0  1]\n",
            " [ 1  8  1]\n",
            " [ 0  3 12]]\n",
            "Accuracy for Combined Model: 0.8333333333333334\n"
          ]
        }
      ],
      "source": [
        "print(\"Classification Report for Combined Model:\")\n",
        "print(classification_report(y_test_combined, y_pred_combined))\n",
        "print(\"Confusion Matrix for Combined Model:\")\n",
        "print(confusion_matrix(y_test_combined, y_pred_combined))\n",
        "print(\"Accuracy for Combined Model:\", accuracy_score(y_test_combined, y_pred_combined))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Predict using the combined model\n",
        "patient_predictions = combined_model(masked_features_patient)\n",
        "\n",
        "# Print the predictions\n",
        "print(patient_predictions)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZJlr41XNWzV2",
        "outputId": "709cbcec-779e-4caf-8344-150a4be9cc71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}